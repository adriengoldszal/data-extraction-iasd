"""
Analyze and visualize geocoding quality from the combined Nominatim + Wikipedia data.
Uses data generated by get_nominatim_locations.py
"""

import json
import math
from pathlib import Path

import matplotlib.pyplot as plt

GEOCODED_PATH = Path("../data/geocoded_locations.json")
OUTPUT_DIR = Path(__file__).parent


def load_geocoded_data(path: Path):
    """Load geocoded locations and categorize by status."""
    data = json.loads(path.read_text(encoding="utf-8"))
    
    # Categorize locations
    both_sources = []      # Have both Nominatim and Wikipedia (with distance)
    nominatim_only = []    # Only Nominatim worked
    wikipedia_only = []    # Only Wikipedia worked
    both_failed = []       # Neither worked
    
    for place, info in data.items():
        has_nom = info.get('nominatim_lat') is not None
        has_wiki = info.get('wikipedia_lat') is not None
        distance = info.get('distance_km')
        
        if has_nom and has_wiki:
            both_sources.append((place, distance, info.get('chosen_source')))
        elif has_nom:
            nominatim_only.append(place)
        elif has_wiki:
            wikipedia_only.append(place)
        else:
            both_failed.append(place)
    
    return {
        'both_sources': both_sources,
        'nominatim_only': nominatim_only,
        'wikipedia_only': wikipedia_only,
        'both_failed': both_failed,
        'total': len(data),
        'raw_data': data,
    }


# Load data
if not GEOCODED_PATH.exists():
    raise SystemExit(f"File not found: {GEOCODED_PATH}\nRun locations/get_locations.py first.")

data = load_geocoded_data(GEOCODED_PATH)

# Extract distances for locations that have both sources
distances_with_labels = [(place, dist) for place, dist, _ in data['both_sources'] if dist is not None]

if not distances_with_labels:
    print("No locations with both Nominatim and Wikipedia coordinates found.")
    print("Charts will not be generated, but statistics will be shown.")
else:
    labels, dists = zip(*distances_with_labels)
    
    # 1) Histogram (km)
    plt.figure(figsize=(10, 6))
    plt.hist(dists, bins=30, edgecolor='black', alpha=0.7)
    plt.xlabel("Distance (km)")
    plt.ylabel("Count")
    plt.title("Distance between Nominatim and Wikipedia coordinates")
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "distance_histogram.png", dpi=150)
    plt.close()
    print(f"Saved: {OUTPUT_DIR / 'distance_histogram.png'}")

    # 2) Histogram (log10 km)
    logd = [math.log10(d) for d in dists if d > 0]
    if logd:
        plt.figure(figsize=(10, 6))
        plt.hist(logd, bins=30, edgecolor='black', alpha=0.7)
        plt.xlabel("log10(distance in km)")
        plt.ylabel("Count")
        plt.title("Distance distribution (log scale)")
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / "distance_histogram_log.png", dpi=150)
        plt.close()
        print(f"Saved: {OUTPUT_DIR / 'distance_histogram_log.png'}")

    # 3) Top 15 outliers bar chart
    top = sorted(distances_with_labels, key=lambda x: x[1], reverse=True)[:15]
    top_labels = [t[0][:30] + ".." if len(t[0]) > 32 else t[0] for t in top][::-1]
    top_dists = [t[1] for t in top][::-1]

    plt.figure(figsize=(12, 8))
    bars = plt.barh(top_labels, top_dists, color='coral', edgecolor='black')
    plt.xlabel("Distance (km)")
    plt.title("Top 15 largest distances between Nominatim and Wikipedia")
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "top_outliers.png", dpi=150)
    plt.close()
    print(f"Saved: {OUTPUT_DIR / 'top_outliers.png'}")

    print("\n All charts saved!")

# ============================================================
# STATISTICS
# ============================================================

print("\n" + "=" * 70)
print("GEOCODING SOURCE COVERAGE")
print("=" * 70)

total = data['total']
both_count = len(data['both_sources'])
nom_only = len(data['nominatim_only'])
wiki_only = len(data['wikipedia_only'])
failed = len(data['both_failed'])
geocoded = total - failed

print(f"\n{'Source Availability':<35} {'Count':<10} {'Percentage'}")
print("-" * 60)
print(f"{'Both Nominatim + Wikipedia':<35} {both_count:<10} {both_count/total*100:>6.1f}%")
print(f"{'Nominatim only':<35} {nom_only:<10} {nom_only/total*100:>6.1f}%")
print(f"{'Wikipedia only':<35} {wiki_only:<10} {wiki_only/total*100:>6.1f}%")
print(f"{'Neither (failed)':<35} {failed:<10} {failed/total*100:>6.1f}%")
print("-" * 60)
print(f"{'Total locations':<35} {total:<10}")
print(f"{'Successfully geocoded':<35} {geocoded:<10} {geocoded/total*100:>6.1f}%")

# Source selection stats
nominatim_chosen = sum(1 for _, _, src in data['both_sources'] if src == 'nominatim')
wikipedia_chosen = sum(1 for _, _, src in data['both_sources'] if src == 'wikipedia')

print(f"\n{'Chosen Source (when both available)':<35} {'Count':<10} {'Percentage'}")
print("-" * 60)
print(f"{'Nominatim (sources agreed)':<35} {nominatim_chosen:<10} {nominatim_chosen/both_count*100 if both_count else 0:>6.1f}%")
print(f"{'Wikipedia (sources diverged)':<35} {wikipedia_chosen:<10} {wikipedia_chosen/both_count*100 if both_count else 0:>6.1f}%")

# ============================================================
# DISTANCE THRESHOLD ANALYSIS
# ============================================================
if distances_with_labels:
    print("\n" + "=" * 70)
    print("DISTANCE ANALYSIS (Nominatim vs Wikipedia)")
    print("=" * 70)
    
    dists_list = [d for _, d in distances_with_labels]
    thresholds = [1, 5, 10, 25, 50, 100, 500, 1000]
    
    print(f"\n{'Threshold':<15} {'Count':<10} {'Percentage':<15} {'Cumulative'}")
    print("-" * 55)
    
    for threshold in thresholds:
        count = sum(1 for d in dists_list if d <= threshold)
        pct = count / len(dists_list) * 100
        print(f"â‰¤ {threshold:>6} km     {count:<10} {pct:>6.1f}%          {pct:>6.1f}%")
    
    above_max = sum(1 for d in dists_list if d > thresholds[-1])
    if above_max > 0:
        print(f"> {thresholds[-1]:>6} km     {above_max:<10} {above_max/len(dists_list)*100:>6.1f}%")
    
    print(f"\nStatistics:")
    print(f"  Locations compared: {len(dists_list)}")
    print(f"  Mean distance:      {sum(dists_list)/len(dists_list):.1f} km")
    print(f"  Median distance:    {sorted(dists_list)[len(dists_list)//2]:.1f} km")
    print(f"  Max distance:       {max(dists_list):.1f} km")
    print(f"  Min distance:       {min(dists_list):.2f} km")

# ============================================================
# TOP 10 LARGEST DIVERGENCES
# ============================================================
if distances_with_labels:
    print("\n" + "=" * 70)
    print("TOP 10 LARGEST DIVERGENCES")
    print("=" * 70)
    
    top10 = sorted(distances_with_labels, key=lambda x: x[1], reverse=True)[:10]
    
    print(f"\n{'Rank':<6} {'Location':<40} {'Distance':<12} {'Used'}")
    print("-" * 70)
    
    for i, (place, dist) in enumerate(top10, 1):
        # Find which source was used
        info = data['raw_data'].get(place, {})
        source = info.get('chosen_source', '?')
        place_short = place[:38] + ".." if len(place) > 40 else place
        print(f"{i:<6} {place_short:<40} {dist:>8.1f} km   {source}")

# ============================================================
# FAILED LOCATIONS
# ============================================================
if data['both_failed']:
    print("\n" + "=" * 70)
    print(f"FAILED LOCATIONS ({len(data['both_failed'])})")
    print("=" * 70)
    
    print("\nLocations that neither Nominatim nor Wikipedia could geocode:")
    for place in data['both_failed'][:15]:
        place_short = place[:60] + ".." if len(place) > 62 else place
        print(f"  - {place_short}")
    if len(data['both_failed']) > 15:
        print(f"  ... and {len(data['both_failed']) - 15} more")

print("\n" + "=" * 70)
